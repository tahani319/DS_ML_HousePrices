---
title: "DS_ML_HousePrice"
author: "Tahani AlHarbi"
date: "12/3/2020"
output: html_document
---

data from: https://www.kaggle.com/shree1992/housedata
the data contains information  about houses in USA 
it contains 18 variables and 4600 observation
my main goal is to building a model to predict the price of a house
based on specific features of the house.



```{r setup, include=FALSE}
knitr::opts_chunk$set(include = FALSE)
```

```{r}
# Helper packages
library(dplyr)     # for data manipulation
library(ggplot2)   # for awesome graphics

# Modeling process packages
library(rsample)   # for resampling procedures
library(caret)     # for resampling and model training
library(h2o)       # for resampling and model training
library(visdat)   # for additional visualizations

# Feature engineering packages
library(caret)    # for various ML tasks
library(recipes)  # for feature engineering tasks
# h2o set-up 
h2o.no_progress()  # turn off h2o progress bars
h2o.init()         # launch h2o
```








```{r}
house <- read.csv("data/data.csv")

```



```{r}
house$statezip <- as.factor(house$statezip)
house$city <- as.factor(house$city)
house$street <- as.factor(house$street)
```



```{r}
#change bathrooms, bedrooms into numeric 

house$price <- as.numeric(house$price)
house$bedrooms <- as.numeric(house$bedrooms)
house$bathrooms <- as.numeric(house$bathrooms)

```


```{r}

#we have statezip so we do not need for columns city, street 
#drop column of country because it has just one value 
#drop column of date beacuse it isn't necessary 


house <- subset( house , select = -c( date,country, street,city) )

str(house)


```


#### **Data head:**
 

```{r}
head(house)
```


#### **Data tail:** 

```{r}
tail(house)
```



```{r}
# initial dimension
dim(house)
# response variable
head(house$price)
```


#### **set h2o environment:**

```{r}
house.h2o <- as.h2o(house)
```

#### **Data Splitting**:

```{r}
# Stratified sampling with the rsample package
set.seed(123) 
split <- initial_split(house, prop = 0.7, 
                       strata = "price")
house_train  <- training(split)
house_test   <- testing(split)
```

#### **To make sure training and test set have the same distributions:**

```{r}
# Do the distributions line up? 
ggplot(house_train, aes(x = price)) + 
  geom_line(stat = "density", 
            trim = TRUE) + 
  geom_line(data = house_test, 
            stat = "density", 
            trim = TRUE, col = "red")
```


#### **Missing valuse:** 

visualize missing values 



```{r}
#viualize missing values 
house %>%
  is.na() %>%
  reshape2::melt() %>%
  ggplot(aes(Var2, Var1, fill=value)) + 
    geom_raster() + 
    coord_flip() +
    scale_y_continuous(NULL, expand = c(0, 0)) +
    scale_fill_grey(name = "", 
                    labels = c("Present", 
                               "Missing")) +
    xlab("Observation") +
    theme(axis.text.y  = element_text(size = 4))
```


obviously we do not have any missing values. 

#### **Resampling:**

apply KNN model to data 

```{r}
# Specify resampling strategy
cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 5
)

# Create grid of hyperparameter values
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))

# Tune a knn model using grid search
knn_fit <- train(
  price ~ ., 
  data = house_train, 
  method = "knn", 
  trControl = cv, 
  tuneGrid = hyper_grid,
  metric = "RMSE"
)
```


see results : 

```{r}
# Print and plot the CV results
knn_fit
```

#### **visualize the fit 

```{r}
ggplot(knn_fit)
```
then i applied features engineering and do KNN model to  my data again 





#### **Features Engineering:**

 applying features enginnering 

  * log transformation for target variable 
  * filter zero variance and near-zero variance features
  * dimension reduction for highly correlated features
  
  target variable is positively skewed so i will use log transformation : 
  
```{r}
# log transformation target variable
house_recipe <- recipe(price ~ ., data = house_train) %>%
  step_log(all_outcomes())
```
  
   
   
    blueprint process : 
 
 Remove near-zero variance features that are categorical.
Ordinal encode quality-based features 
Center and scale all numeric features.
Perform dimension reduction by applying PCA to all numeric features.

```{r}
blueprint <- recipe(price ~ ., data = house_train) %>%
  step_nzv(all_nominal())  %>%
  step_integer(matches("Qual|Cond|QC|Qu")) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_pca(all_numeric(), -all_outcomes())
  
blueprint
```
 
 
 Estimate parameters based on the training data of interest: 
  
```{r}
prepare <- prep(blueprint, training = house_train) #Estimate parameters based on the training data of interest
prepare
```
  
  
apply our blueprint to new data with baked: 
  
  
```{r}
baked_train <- bake(prepare, new_data = house_train) #apply our blueprint to new data with baked 
baked_test <- bake(prepare, new_data = house_test)
baked_train
```
  
  
  
  
  

```{r}
blueprint <- recipe(price ~ ., data = house_train) %>%
  step_nzv(all_nominal()) %>%
  step_integer(matches("Qual|Cond|QC|Qu")) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
```

apply resample method , hyper grid (hyperparameter) and supply blueprint (data proccessing) 

```{r}
# Specify resampling plan
cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 5
)

# Construct grid of hyperparameter values
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))

# Tune a knn model using grid search
knn_fit2 <- train(
  blueprint, 
  data = house_train, 
  method = "knn", 
  trControl = cv, 
  tuneGrid = hyper_grid,
  metric = "RMSE"
)
```

see the model result : 

```{r}
# print model results
knn_fit2

```


visualize cross validation result: 

```{r}
# plot cross validation results
ggplot(knn_fit2)
```


#### Applay Linear regression: 



##### simple linear regression: 

one predictor 

```{r}
model1 <- lm(price ~ sqft_living , data = house_train)
summary(model1)
```
 
#### ** multiple linear regression**
 
 
 multiple linear regression with 2 predictors 
s 

```{r}
# OLS model with two predictors
model2 <- lm(price ~ sqft_living + yr_built, data = house_train)
summary(model2)
```


 and with all main features: 


```{r}
# include all possible main effects
model3 <- lm(price ~ ., data = house_train)
summary(model3)
```



# Assessing model accuracy 

fit 3 models  

1. a single predictor 
2. two predictors 
3. all possible main effect predictors. 



```{r}
# create a resampling method
cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 5
  )

# model 1 CV
set.seed(123)
(cv_model1 <- train(
  price ~ sqft_living, 
  data = house_train, 
  method = "lm", #<<
  trControl = cv)
)
```



```{r}
# model 2 CV
set.seed(123)
cv_model2 <- train(
  price ~ sqft_living + yr_built, 
  data = house_train, 
  method = "lm",
  trControl = cv
)

# Extract out of sample performance measures
summary(resamples(list(
  model1 = cv_model1, 
  model2 = cv_model2
)))
```




#### # Model concerns - multicollinearity

 I saw in EDA that sqft_above are highly correlated with sqft_living


```{r}
m1 <- lm(price ~ sqft_living + sqft_above , data = house_train) # 2 predictors

coef(m1) 

```

visulaize 


```{r}
# The are highly correlated
cor(house_train$sqft_living, house_train$sqft_above )

# simple plot
plot(house_train$sqft_living, house_train$sqft_above )
```
they are correlated 


# Principal Component Regression


so I will use principle component regression to avoid multicollinearity problems 


```{r}
hyper_grid <- expand.grid(ncomp = seq(2, 40, by = 2))

# 2. PCR
set.seed(123)
cv_pcr <- train(
  price ~ ., 
  data = house_train, 
  trControl = cv,
  method = "pcr", #<<
  preProcess = c("zv", "center", "scale"), #<<
  tuneGrid = hyper_grid, #<<
  metric = "RMSE"
  )

# model with lowest RMSE
cv_pcr$bestTune

cv_pcr$results %>%
  filter(ncomp == as.numeric(cv_pcr$bestTune))

# plot cross-validated RMSE
plot(cv_pcr)
```
#### **Tuning:**


```{r}
# 1. hypergrid
p <- length(house_train) - 1
hyper_grid <- expand.grid(ncomp = seq(2, 80, length.out = 10)) 

# 2. PCR
set.seed(123)
cv_pcr <- train(
  price ~ ., 
  data = house_train, 
  trControl = cv,
  method = "pcr", 
  preProcess = c("zv", "center", "scale"), 
  tuneGrid = hyper_grid, 
  metric = "RMSE"
  )

# RMSE
cv_pcr$results %>%
  filter(ncomp == cv_pcr$bestTune$ncomp)

# plot cross-validated RMSE
plot(cv_pcr)
```

#### **Partial least squares:**

Using PLS 

```{r}
# PLS
set.seed(123)
cv_pls <- train(
  price ~ ., 
  data = house_train, 
  trControl = cv,
  method = "pls", #<<
  preProcess = c("zv", "center", "scale"),
  tuneGrid = hyper_grid,
  metric = "RMSE"
  )

# model with lowest RMSE
cv_pls$bestTune

cv_pls$results %>%
  filter(ncomp == as.numeric(cv_pls$bestTune))

# plot cross-validated RMSE
plot(cv_pls)
```







#### **Model Comparison**

compare all models 


```{r}
results <- resamples(list(
  OLS  = cv_model3, 
  PCR  = cv_pcr, 
  PLS  = cv_pls
  ))

summary(results)$statistics$RMSE

# plot results
p1 <- bwplot(results, metric = "RMSE")
p2 <- dotplot(results, metric = "RMSE")
gridExtra::grid.arrange(p1, p2, nrow = 1)  #comparison all models 
```



























