---
title: "DS_ML_HousePrice"
author: "Tahani AlHarbi"
date: "12/3/2020"
output: html_document
---

data from: https://www.kaggle.com/shree1992/housedata
the data contains information  about selling houses in USAin 2014 
it contains 18 variables and 4600 observation
my main goal is to building a model to predict the price of a house
based on specific features of the house.

this ML project include 
 
 
 * EDA(Explanatory Data Analysis)
 * Linear regression
 * Partial least square regression
 * 
 *  Decision Tree Model
 * Model Validation

```{r setup, include=TRUE}
knitr::opts_chunk$set(include = FALSE)
```

```{r}
# Helper packages
library(dplyr)     # for data manipulation
library(ggplot2)   # for awesome graphics

# Modeling process packages
library(rsample)   # for resampling procedures
library(caret)     # for resampling and model training
library(h2o)       # for resampling and model training
library(visdat)   # for additional visualizations

# Feature engineering packages
library(caret)    # for various ML tasks
library(recipes)  # for feature engineering tasks
# h2o set-up 
h2o.no_progress()  # turn off h2o progress bars
h2o.init()         # launch h2o
```








```{r}
house <- read.csv("data/data.csv")

```



```{r}
house$statezip <- as.factor(house$statezip)
house$city <- as.factor(house$city)
house$street <- as.factor(house$street)
```



```{r}
#change bathrooms, bedrooms into numeric 


house$bedrooms <- as.numeric(house$bedrooms)
house$bathrooms <- as.numeric(house$bathrooms)

```


```{r}

#we have statezip so we do not need for columns city, street 
#drop column of country because it has just one value 
#drop column of date beacuse it isn't necessary 


house <- subset( house , select = -c( date,country, street,city) )

str(house)


```


#### **Data head:**
 

```{r}
head(house)
```


#### **Data tail:** 

```{r}
tail(house)
```



```{r}
# initial dimension
dim(house)
# response variable
head(house$price)
```



see distribution of target variable:

```{r}
#see distribution of target variable
ggplot(house, aes(price)) +
  geom_density() 

```









#### **set h2o environment:**

```{r}
house.h2o <- as.h2o(house)
```

#### **Data Splitting**:

```{r}
# Stratified sampling with the rsample package
set.seed(123) 
split <- initial_split(house, prop = 0.7, 
                       strata = "price")
house_train  <- training(split)
house_test   <- testing(split)
```

#### **To make sure training and test set have the same distributions:**

```{r}
# Do the distributions line up? 
ggplot(house_train, aes(x = price)) + 
  geom_line(stat = "density", 
            trim = TRUE) + 
  geom_line(data = house_test, 
            stat = "density", 
            trim = TRUE, col = "red")
```
#### **Features Engineering:**

 applying features enginnering 
  * imputation outliers
  * Missing values
  * log transformation for target variable 
  * filter zero variance and near-zero variance features
  * dimension reduction for highly correlated features



first let's explore outliers values in price  data : 

```{r}
ggplot(house_train, mapping = aes(x = statezip, y = price, fill = statezip)) + 
  geom_boxplot(outlier.colour = "red", outlier.shape = 5, outlier.size = 4) 
```
actually we have outliers in data 

#### Outlier detection and normalizing:


```{r}
# outlier detection and normalizing
outlier_norm <- function(x){
   qntile <- quantile(x, probs=c(.25, .75))
   caps <- quantile(x, probs=c(.05, .95))
   H <- 1.5 * IQR(x, na.rm = T)
   x[x < (qntile[1] - H)] <- caps[1]
   x[x > (qntile[2] + H)] <- caps[2]
   return(x)}

house_train$price=outlier_norm(house_train$price)
ggplot(house_train, mapping = aes(x = statezip, y = price, fill = statezip)) + 
  geom_boxplot(outlier.colour = "red", outlier.shape = 8, outlier.size = 4) 
```


  

#### **Missing values:** 

visualize missing values 



```{r}
#viualize missing values 
house %>%
  is.na() %>%
  reshape2::melt() %>%
  ggplot(aes(Var2, Var1, fill=value)) + 
    geom_raster() + 
    coord_flip() +
    scale_y_continuous(NULL, expand = c(0, 0)) +
    scale_fill_grey(name = "", 
                    labels = c("Present", 
                               "Missing")) +
    xlab("Observation") +
    theme(axis.text.y  = element_text(size = 4))
```

obviously we do not have any missing values. 



#### **blueprint process:** 
 
Remove near-zero variance features that are categorical.
Ordinal encode quality-based features 
Center and scale all numeric features.
Perform dimension reduction by applying PCA to all numeric features.


```{r}
blueprint <- recipe(price ~ ., data = house_train) %>%
  step_nzv(all_nominal())  %>%
  step_integer(matches("Qual|Cond|QC|Qu")) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_pca(all_numeric(), -all_outcomes())
  
blueprint
```


 Estimate parameters based on the training data of interest:

```{r}
prepare <- prep(blueprint, training = house_train) #Estimate parameters based on the training data of interest
prepare
```

apply our blueprint to new data with baked: 

```{r}
baked_train <- bake(prepare, new_data = house_train) #apply our blueprint to new data with baked 
baked_test <- bake(prepare, new_data = house_test)
baked_train
```




```{r}
blueprint <- recipe(price ~ ., data = house_train) %>%
  step_nzv(all_nominal()) %>%
  step_integer(matches("Qual|Cond|QC|Qu")) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
```


#### **Resampling:**

apply KNN model to the  data 


```{r}
# Specify resampling plan
cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 5
)

# Construct grid of hyperparameter values
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))

# Tune a knn model using grid search
knn_fit <- train(
  blueprint, 
  data = house_train, 
  method = "knn", 
  trControl = cv, 
  tuneGrid = hyper_grid,
  metric = "RMSE"
)
```

see results :

```{r}
# print model results
knn_fit
```


#### **visualize the fit 

```{r}
# plot cross validation results
ggplot(knn_fit)
```

#### Applay Linear regression: 



##### simple linear regression: 

I wanted to model a linear relationship between living area of home (sqft_living) and  price. 

one predictor 

```{r}
model1 <- lm(price ~ sqft_living , data = house_train) #linear regression 
#one predictor 

summary(model1)
```

The estimated coefficients from our model are βˆ0=23391.49 and βˆ1= 248.12. 
we estimate that the mean selling price increases by 248.12 for each additional one square foot of living area.

 
#### ** multiple linear regression**
 
we wanted to see if the living area of house and the year was it build have affect on price
so we used multiple linear regression with 2 predictors living area of house (sqft_living)
 and year of built (yr_built)


```{r}
# OLS model with two predictors
model2 <- lm(price ~ sqft_living + yr_built, data = house_train)
summary(model2)
```




 and with all main features: 


```{r}
# include all possible main effects
model3 <- lm(price ~ ., data = house_train)
summary(model3)
```



# Assessing model accuracy 

fit 3 models  

1. Linear regression,  a single predictor 
2. Multiple linear regression, two predictors 
3. all possible main effect predictors. 



```{r}
# create a resampling method
cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 5
  )

# model 1 CV
set.seed(123)
(cv_model1 <- train(
  price ~ sqft_living, 
  data = house_train, 
  method = "lm", 
  trControl = cv)
)
```


 RMSE is $199629.5 (average RMSE across the 10 CV folds). which means about $199629.5 off from the actual sale price.




```{r}
# model 2 CV
set.seed(123)
cv_model2 <- train(
  price ~ sqft_living + yr_built, 
  data = house_train, 
  method = "lm",
  trControl = cv
)

#model3
set.seed(123)
cv_model3 <- train(
  price ~ ., 
  data = house_train, 
  method = "lm",
  trControl = cv
)

```
```{r}
# Extract out of sample performance measures
summary(resamples(list(
  model1 = cv_model1, 
  model2 = cv_model2,
  model3 = cv_model3
)))
```




#### # Model concerns - multicollinearity

 I saw in EDA that sqft_above and sqft_living are correlated


```{r}
m1 <- lm(price ~ sqft_living + sqft_above , data = house_train) # 2 predictors

coef(m1) 

```


see the correlation:

```{r}
# The are highly correlated
cor(house_train$sqft_living, house_train$sqft_above )

# simple plot
plot(house_train$sqft_living, house_train$sqft_above )
```
they are correlated 


# Principal Component Regression


so I use principle component regression to avoid multicollinearity  


```{r}
hyper_grid <- expand.grid(ncomp = seq(2, 40, by = 2))

# 2. PCR
set.seed(123)
cv_pcr <- train(
  price ~ ., 
  data = house_train, 
  trControl = cv,
  method = "pcr", 
  preProcess = c("zv", "center", "scale"), #<<
  tuneGrid = hyper_grid, #<<
  metric = "RMSE"
  )

# model with lowest RMSE
cv_pcr$bestTune

cv_pcr$results %>%
  filter(ncomp == as.numeric(cv_pcr$bestTune))

# plot cross-validated RMSE
plot(cv_pcr)
```
#### **Tuning:**


```{r}
# 1. hypergrid
p <- length(house_train) - 1
hyper_grid <- expand.grid(ncomp = seq(2, 80, length.out = 10)) 

# 2. PCR
set.seed(123)
cv_pcr <- train(
  price ~ ., 
  data = house_train, 
  trControl = cv,
  method = "pcr", 
  preProcess = c("zv", "center", "scale"), 
  tuneGrid = hyper_grid, 
  metric = "RMSE"
  )

# RMSE
cv_pcr$results %>%
  filter(ncomp == cv_pcr$bestTune$ncomp)

# plot cross-validated RMSE
plot(cv_pcr)
```

#### **Partial least squares:**

it Similar to PCR, this technique also constructs a set of linear combinations of the inputs for regression, it uses the response variable to aid the construction of the principal components.



```{r}
# using PLS
set.seed(123)
cv_pls <- train(
  price ~ ., 
  data = house_train, 
  trControl = cv,
  method = "pls", 
  preProcess = c("zv", "center", "scale"),
  tuneGrid = hyper_grid,
  metric = "RMSE"
  )

# model with lowest RMSE
cv_pls$bestTune

cv_pls$results %>%
  filter(ncomp == as.numeric(cv_pls$bestTune))

# plot cross-validated RMSE
plot(cv_pls)
```
#### **Feature interpretaion:**
  
Extract and plot 20 of the most important variable 
  
```{r}
vip(cv_pls, num_features = 20, method = "model") #Extract and plot 20 of the most important variable 
```


#### **Model Comparison**

compare all models OLS, PCR and PLS


```{r}
#compare all models

results <- resamples(list(
  OLS  = cv_model3, 
  PCR  = cv_pcr, 
  PLS  = cv_pls
  ))

summary(results)$statistics$RMSE

# plot results
p1 <- bwplot(results, metric = "RMSE")
p2 <- dotplot(results, metric = "RMSE")
gridExtra::grid.arrange(p1, p2, nrow = 1)  
```



#### **Decision trees:**

I built decision trees model to predict house price depending on house features 

##### **Load packages:**

```{r}
# Helper packages
library(dplyr)       # for data wrangling
library(ggplot2)     # for awesome plotting

# Modeling packages
library(rpart)       # direct engine for decision tree application
library(caret)       # meta engine for decision tree application

# Model interpretability packages
install.packages("rpart.plot")
library(rpart.plot)  # for plotting decision trees
library(vip)         # for feature importance
library(pdp)         # for feature effects
```


fit a regression tree using anova method 


```{r}
#fit a regression tree using rpart

house_dt1 <- rpart(
  formula = price ~ .,
  data    = house_train,
  method  = "anova" #to fiiting regression tree 
)
```



show decision tree output: 


```{r}
house_dt1  #show decision tree output
```
it starts with 3220 observations at root node the first variable splits is statezip which is the important predictor 
to home price 

#### **Visualize regression tree:** 

```{r}
rpart.plot(house_dt1) #plot regression tree
```
plot cross validated error summary :

```{r}
plotcp(house_dt1) #plot cross validated error summary
```



```{r}
house_dt2 <- rpart(
    formula = price ~ .,
    data    = house_train,
    method  = "anova", 
    control = list(cp = 0, xval = 10)
)

plotcp(house_dt2)
abline(v = 11, lty = "dashed")
```
#### **cross validation results: 

```{r}
# rpart cross validation results

house_dt1$cptable
```

#### **cross validation result**:


```{r}
# caret cross validation results
house_dt3 <- train(
  price ~ .,
  data = house_train,
  method = "rpart",
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 20
)

```
 
#### **visulaziation**: 

visualize RMSE 
 
```{r}
ggplot(house_dt3) #plot RMSE cross validation
```
#### **Feature interpretation:**


visualize 30 of top features in decision tree


```{r}
vip(house_dt3, num_features = 30, geom = "point")
```

#### Construct partial dependence plots:

```{r}
p1 <- partial(house_dt3, pred.var = "sqft_living") %>% 
  ggplot(aes(sqft_living, yhat)) +
  geom_line()

p2 <- partial(house_dt3, pred.var = "yr_built") %>% 
  ggplot(aes(yr_built, yhat)) +
  geom_line()

p3 <- partial(house_dt3, pred.var = c("sqft_living", "yr_built")) %>% 
  plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, 
              colorkey = TRUE, screen = list(z = -20, x = -60))

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```

